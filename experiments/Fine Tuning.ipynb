{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab93d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(text):\n",
    "    \"\"\"\n",
    "    \"Return the length of the text.\"\n",
    "\n",
    "    The function takes a single argument, text, and returns the length of the text\n",
    "\n",
    "    :param text: The text to be analyzed\n",
    "    :return: The length of the text.\n",
    "    \"\"\"\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d60fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterSubstitute(text):\n",
    "    \"\"\"\n",
    "    It takes a string, removes all newline characters, removes all instances of the string \"Hook 1\", and replaces all\n",
    "    multiple spaces with a single space\n",
    "\n",
    "    :param text: the text to be processed\n",
    "    :return: the text with the newlines, carriage returns, and Hook 1 removed.\n",
    "    \"\"\"\n",
    "    regex = r'\\n|\\r|Hook 1|[0-9]|chorus'\n",
    "    text = re.sub(regex, \" \", text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f91e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    \"\"\"\n",
    "    * The function takes in a string and returns a new string which doesn't contain any punctuation.\n",
    "\n",
    "    * For example, calling the function with the string `\"Let's try, Mike.\"` should return `\"Lets try Mike\"`\n",
    "\n",
    "    :param text: The text whose punctuations are to be removed\n",
    "    :return: The text stripped of punctuation marks\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb770f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyStopWords(text):\n",
    "    \"\"\"a function for removing the stopword\"\"\"\n",
    "    #stop_words = ['a', 'an', 'above', 'and', 'any', 'as', 'at', 'of', 'that', 'the', 'to']\n",
    "    stop_words = stopwords.words('english')\n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a074f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    \"\"\"\n",
    "    It takes a string of text, splits it into words, and then returns a string of text where each word is stemmed\n",
    "\n",
    "    :param text: The text that you want to stem\n",
    "    :return: the stemmed words in the text.\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceef9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF1(file_name):\n",
    "    \"\"\"\n",
    "    It reads the csv file, renames the columns, creates a binary column for the mood, and creates a column for the\n",
    "    length of the lyrics\n",
    "\n",
    "    :param file_name: the name of the file you want to import\n",
    "    :return: A dataframe with the columns: lyrics, song name, valence, mood, length\n",
    "    \"\"\"\n",
    "    # Import CSV file\n",
    "    df = pd.read_csv(file_name, sep=',', index_col=[0])\n",
    "\n",
    "    # Rename columns names: \"seq\": \"lyrics\", \"song\": \"song name\", \"label\":\"valence\"\n",
    "    df.rename(columns={\"seq\": \"lyrics\", \"song\": \"song name\", \"label\": \"valence\"}, inplace=True)\n",
    "\n",
    "    # Create 'length' column that represent the lyrics' number of words\n",
    "    df['length'] = df['lyrics'].apply(length)\n",
    "    df['length_log'] = np.log(df['length'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365b4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleansing(df):\n",
    "    \"\"\"\n",
    "    It takes in a dataframe, cleanses the lyrics' column, and returns a dataframe.\n",
    "\n",
    "    :param df: the dataframe that contains the lyrics\n",
    "    :return: A dataframe with the lyrics' column cleaned.\n",
    "    \"\"\"\n",
    "    # Drop Duplicates\n",
    "    df = df.drop_duplicates(subset=['lyrics']) \n",
    "    \n",
    "    # Substitute special regex/characters\n",
    "    df['lyrics'] = df['lyrics'].apply(characterSubstitute)\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['lyrics'] = df['lyrics'].apply(removePunctuation)\n",
    "\n",
    "    # Lowercase all words\n",
    "    df['lyrics'] = df['lyrics'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Keep song with lyrics length between 500 and 2000\n",
    "    df = df[(df['length'] < 2000) & (df['length'] > 500)] \n",
    "    \n",
    "    df = df[(df['valence'] > 0.85) | (df['valence'] < 0.15)]\n",
    "    \n",
    "    # Create binary column: 1 represent \"happy\" mood while 0 represent \"sad column\"\n",
    "    df['Mood'] = np.where(df['valence'] > 0.85, 1, 0)\n",
    "\n",
    "    # Remove StopWords\n",
    "    df['lyrics'] = df['lyrics'].apply(applyStopWords)\n",
    "\n",
    "    # Stemming\n",
    "    df['lyrics'] = df['lyrics'].apply(stemming)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca40050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampling(df):\n",
    "    \"\"\"\n",
    "    It takes a dataframe as input, and returns a dataframe with the\n",
    "    same number of rows as the input dataframe, but with the same number of rows for each class\n",
    "\n",
    "    :param df: The dataframe you want to down sample\n",
    "    :return: A dataframe with the same number of negative and positive moods.\n",
    "    \"\"\"\n",
    "    requires_n = df['Mood'].value_counts().min()\n",
    "    print(requires_n)\n",
    "    requires_n = 8000\n",
    "    negative_mood = df[df['Mood'] == 0].sample(n=requires_n)\n",
    "    positive_mood = df[df['Mood'] == 1].sample(n=requires_n)\n",
    "\n",
    "    down_sampling_data = pd.concat([negative_mood, positive_mood])\n",
    "\n",
    "    # The frac keyword argument specifies the fraction of rows to return to the random sample, so frac=1 means to\n",
    "    # return all rows (in random order).\n",
    "    down_sampling_data = down_sampling_data.sample(frac=1)\n",
    "    return down_sampling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e98ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handlingData():\n",
    "    \"\"\"\n",
    "    It takes the raw data, cleans it, and down sample it\n",
    "    :return: ml_data\n",
    "    \"\"\"\n",
    "    file_name = '1_First Data.csv'\n",
    "    data = createDF1(file_name)\n",
    "    cleaned_data = dataCleansing(data)\n",
    "    ml_data = downSampling(cleaned_data)\n",
    "    ml_data.to_csv(\"2_ML data.csv\")\n",
    "\n",
    "    return ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef311cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It imports the pandas library and renames it to pd.\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the train_test_split, TfidfVectorizer, and Pipeline functions from the sklearn library.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Importing the models that we are going to use.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Importing the functions that we are going to use to evaluate the models.\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# It imports the pyplot module from the matplotlib library and renames it to plt.\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93c94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF2(file_name):\n",
    "    \"\"\"\n",
    "    It reads the csv file, renames the columns, creates a binary column for the mood, and creates a column for the\n",
    "    length of the lyrics\n",
    "\n",
    "    :param file_name: the name of the file you want to import\n",
    "    :return: A dataframe with the columns: lyrics, song name, valence, mood, length\n",
    "    \"\"\"\n",
    "    # Import CSV file\n",
    "    df = pd.read_csv(file_name, usecols=['lyrics', 'Mood'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c497ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(df):\n",
    "    \"\"\"\n",
    "    It takes in a dataframe, and returns a dataframe with the lyrics column removed, and replaced with a TFIDF vectorized\n",
    "    version of the lyrics column.\n",
    "\n",
    "    :param df: the dataframe that contains the lyrics' column\n",
    "    :return: A dataframe with the lyrics vectorized and the mood and length_log columns.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    x = vectorizer.fit_transform(df['lyrics'])\n",
    "    \n",
    "    print(vectorizer.get_feature_names_out())\n",
    "    \n",
    "    vectorizer_df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "    df.drop('lyrics', axis=1, inplace=True)  # Consist of 'mood' & 'length_log'\n",
    "    result = pd.concat([df, vectorizer_df], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e55d634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df):\n",
    "    \"\"\"\n",
    "    It takes in a dataframe, drops the 'Mood' column, and then splits the dataframe into training and testing data\n",
    "\n",
    "    :param df: the dataframe\n",
    "    :return: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    x = df.drop(columns='Mood', axis=1)\n",
    "    y = df['Mood']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    x_train.to_csv(\"x_train.csv\")\n",
    "    x_test.to_csv(\"x_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a44c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, x_train, y_train):\n",
    "    \"\"\"\n",
    "    It takes a model, a training set, and a training label, and returns a trained model\n",
    "\n",
    "    :param model: the model to be trained\n",
    "    :param x_raw_train: the training data\n",
    "    :param y_train: the labels of the training data\n",
    "    :return: The classifier is being returned.\n",
    "    \"\"\"\n",
    "    classifier = Pipeline([('clf', model)])\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35679bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicModelPipeline():  \n",
    "    \"\"\"\n",
    "    It takes in a dataframe, performs TFIDF on it, splits it into train and test sets, trains a bunch of models on the train\n",
    "    set, and then prints out the metrics for each model on the test set.\n",
    "\n",
    "    :param ml_data: the dataframe that contains the text and the labels\n",
    "    \"\"\"\n",
    "    file_name = '2_ML Data.csv'\n",
    "    ml_data = createDF2(file_name)\n",
    "    df_after_TFIDF = TFIDF(ml_data)\n",
    "    df_after_TFIDF.to_csv(\"3_Optimization Data.csv\")\n",
    "    x_train, x_test, y_train, y_test = trainTestSplit(df_after_TFIDF)\n",
    "    \n",
    "    ml_models = {\n",
    "        'LogReg': LogisticRegression(),\n",
    "        'LinearSVC': LinearSVC(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "        'RandomForest': RandomForestClassifier(),\n",
    "        'SVM': SVC(),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),\n",
    "        'Naive Bayes': MultinomialNB()\n",
    "    }\n",
    "\n",
    "\n",
    "    df_metrics = pd.DataFrame([])\n",
    "\n",
    "    for model in ml_models:\n",
    "        classifier = trainModel(ml_models[model], x_train, y_train)\n",
    "        y_predict = classifier.predict(x_test)\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = accuracy_score(y_test, y_predict)\n",
    "        metrics['precision'] = precision_score(y_test, y_predict)\n",
    "        metrics['recall'] = recall_score(y_test, y_predict)\n",
    "        metrics['f1'] = f1_score(y_test, y_predict, average='macro')\n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame(metrics, index=[model]).T], axis=1)\n",
    "   \n",
    "    print(df_metrics)\n",
    "    df_metrics.to_csv(\"4_Models Results Before Fine Tuning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9d56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the pandas library and giving it the alias pd.\n",
    "import json\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the numpy library and giving it the alias np.\n",
    "import numpy as np\n",
    "# Importing the mean function from the numpy library.\n",
    "from numpy import mean\n",
    "\n",
    "# Importing the necessary libraries for the model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing the standard error of the mean function from the scipy library.\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Importing the pyplot module from the matplotlib library.\n",
    "from matplotlib import pyplot, pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd90f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF(file_name):\n",
    "    \"\"\"\n",
    "    It reads the csv file, renames the columns, creates a binary column for the mood, and creates a column for the\n",
    "    length of the lyrics\n",
    "\n",
    "    :param file_name: the name of the file you want to import\n",
    "    :return: A dataframe with the columns: lyrics, song name, valence, mood, length\n",
    "    \"\"\"\n",
    "    # Import CSV file\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe4eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(x, y, repeats):\n",
    "    \"\"\"\n",
    "    It creates a logistic regression model and evaluates it using repeated k-fold cross-validation\n",
    "\n",
    "    :param x: The input data\n",
    "    :param y: The target variable\n",
    "    :param repeats: the number of times to repeat the cross-validation procedure\n",
    "    :return: The accuracy of the model\n",
    "    \"\"\"\n",
    "    # prepare the cross-validation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    # create model\n",
    "    model = RandomForestClassifier()\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f4cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(x, y):\n",
    "    \"\"\"\n",
    "    It will evaluate the model using a given number of repeats and summarize the results\n",
    "\n",
    "    :param x: The input data\n",
    "    :param y: The target variable\n",
    "    \"\"\"\n",
    "    # configurations to test\n",
    "    repeats = range(1, 6)\n",
    "    results = list()\n",
    "    print(\"Cross Validation scores (Mean & Standard Deviation)\")\n",
    "    for r in repeats:\n",
    "        # evaluate using a given number of repeats\n",
    "        scores = evaluate_model(x, y, r)\n",
    "        # summarize\n",
    "        print('>%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))\n",
    "        # store\n",
    "        results.append(scores)\n",
    "        \n",
    "    # plot the results\n",
    "    pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "    pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da7c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperRF(x_train, x_test, y_train, y_test):\n",
    "    max_depth = [32, 64]\n",
    "    n_estimators = [128, 256]\n",
    "    \n",
    "    # bootstrap= [True, False]\n",
    "    # max_depth= [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "    # max_features= ['auto', 'sqrt']\n",
    "    # min_samples_leaf= [1, 2, 4]\n",
    "    # min_samples_split= [2, 5, 10]\n",
    "    # n_estimators= [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "\n",
    "    param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "\n",
    "    # Build the grid search\n",
    "    dfrst = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators)\n",
    "    grid = GridSearchCV(estimator=dfrst, param_grid=param_grid, cv=10)\n",
    "    grid_results = grid.fit(x_train, y_train)\n",
    "\n",
    "    # Summarize the results in a readable format\n",
    "    print(\"Best: {0}, using {1}\".format(grid_results.cv_results_['mean_test_score'], grid_results.best_params_))\n",
    "    results_df = pd.DataFrame(grid_results.cv_results_)\n",
    "    print(results_df)\n",
    "    results_df.to_csv('Grid Search Results.csv')\n",
    "\n",
    "    # Writing Json data into a file\n",
    "    with open('Random forest Best Parameters.txt', 'w') as outfile:\n",
    "        json.dump(grid_results.best_params_, outfile)\n",
    "\n",
    "    # Extract the best decision forest\n",
    "    best_clf = grid_results.best_estimator_\n",
    "    y_pred = best_clf.predict(x_test)\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Create heatmap from the confusion matrix\n",
    "\n",
    "    class_names = [False, True]  # name  of classes\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = [0.5, 1.5]\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9cf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuning():\n",
    "    file_name = '3_Optimization Data.csv'\n",
    "    RF_data = createDF(file_name)\n",
    "\n",
    "    x = RF_data.drop(columns='Mood', axis=1)\n",
    "    y = RF_data['Mood']\n",
    "\n",
    "    # crossValidation(x, y)\n",
    "    print(\"step 1\")\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    x_train = pd.read_csv(\"x_train.csv\")\n",
    "    x_test = pd.read_csv(\"x_test.csv\")\n",
    "    y_train = pd.read_csv(\"y_train.csv\")\n",
    "    y_test = pd.read_csv(\"y_test.csv\")\n",
    "    \n",
    "    model = HyperRF(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601535a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n"
     ]
    }
   ],
   "source": [
    "# handlingData()\n",
    "# basicModelPipeline()\n",
    "fineTuning()\n",
    "\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6180d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a383b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [None, 32]\n",
    "n_estimators = [100, 256]  \n",
    "bootstrap= [True, False]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for depth in max_depth:\n",
    "    for estimator in n_estimators:\n",
    "        for boostrap_para in bootstrap:\n",
    "            classifier = trainModel(RandomForestClassifier(bootstrap=boostrap_para, max_depth=depth, n_estimators=estimator), x_train, y_train)\n",
    "            y_predict = classifier.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_predict)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'bootstrap':boostrap_para, 'max_depth':depth, 'n_estimators':estimator}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
